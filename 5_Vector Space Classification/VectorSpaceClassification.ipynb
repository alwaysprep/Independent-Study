{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Space Classification Review\n",
    "\n",
    "In this notebook, we are going to learn new techniques to classify documents in the context of vector space. The general assumption of the vector space model is that the documents in the same class would be in the contiguous region in vector space and there would not be any interception between two classes. This hypothesis is called <i>contiguity hypothesis</i>. All the vector space algorithms are built on top of this hypothesis.\n",
    " \n",
    "We are going to learn two way of vector space classification. First one is Rocchio and the second one is K-nearest neighbors (kNN). Rocchio classification divides the vector space into regions by using center of classes as shown in Fig. 1. Center of a class is calculated by averaging documents place in the vector space. It is very simple but somewhat inefficient algorithm for the classification task.\n",
    "<br>\n",
    "\n",
    "\n",
    "<img src=\"./rocchio.png\" width=\"250px\">\n",
    "\\begin{align}\n",
    "    \\text {Fig. 1. Rocchio classification with three classes.}\n",
    "\\end{align}\n",
    "\n",
    "Centroids are calculated like:\n",
    "\n",
    "\\begin{align}\n",
    "\\vec{\\mu}(c) = \\frac{1}{|D_c|} \\sum_{d \\epsilon D_c} \\vec{v}(d) \n",
    "\\end{align}\n",
    "\n",
    "where $D_c$ is the documents in class c, $\\vec{v}$ is length normalization function. In the end, we get the average of length normalized document vectors of a class. By using the centroids of classes, we can draw lines that partition the vector space into regions. Running time complexity of Rocchio classification is $O(|D| \\text{ } |V|)$, which is same with naive bayes classification.\n",
    "\n",
    "<br>\n",
    "KNN finds k nearest neighbors and chose the class with highest number of nearest neighbor. For example, if k=8 and document d has 3 neighbors from c1 and 4 neighbors from c2 and 1 neighbor from c3, the algorithm would assign class c2 since it has the most neighbors for k=8.\n",
    "<br>\n",
    "\n",
    "<img src=\"./knn.png\" width=\"250px\">\n",
    "\\begin{align}\n",
    "    \\text {Fig. 2.   KNN classification with two classes. Algorithm chooses red class since it has more neighbors than blue.}\n",
    "\\end{align}\n",
    "<br>\n",
    "\n",
    "KNN does not need training process. We can start testing the model right off the bat. \n",
    "Since finding the nearest neighbor takes n time, the algorithm is not very efficient compared to other classification methods. To calculate distance we can use euclidian distance or cosine distance. Cosine distance has a advantage over others, which is that it would allow us to rank the documents in the same class, even if 2 document has the same number of neighbors in that class. Although the complexity of algorithm is not very efficient, the number of class would not change the running time. That's why kNN would be a better choice as a multi-class classifier. Since kNN memorizes the training data, it is called <i>memory-based learning</i>. <br>\n",
    "In machine learning, it is always better to have more data than choosing complex algorithms for getting better accuracy. Unfortuantely, running time complexity is mostly dependent on the training set in kNN, it would take much more time than other algorithms.  \n",
    "\n",
    "<h2>Linear Versus nonlinear classifiers</h2>\n",
    "\n",
    "We are going to discuss naive bayes classifier and Rocchio classifier as linear classifiers. Linear classifiers divide the vector space with a line, we call that line a <i>class boundary</i>.\n",
    "<br>\n",
    "In reality, we cannot truly separate two classes with linear classifiers due to noisy documents. Noisy documents can happen because of human errors like giving misleading answers to the questionnaire. Even though the classes are linearly separable, there would be an infinite number of possible linear separators. That's why linear classification is very hard to do. \n",
    "\n",
    "\n",
    "<div id=\"header\" style=\"margin: 0 auto; width: 70%;\">\n",
    "    <div class=\"column2\" style=\"float: left;width: 300px;height:250px;text-align: left;\"> <img src=\"linear.png\" style=\"width: 300px;height:250px;\" /></div>\n",
    "    <div class=\"column1\" style=\"float: left;width: 250px;height:250px;text-align: left;\"> <img src=\"nonlinear.png\" style=\"width: 250px;height:250px;\"/></div>\n",
    "    <div style=\"clear:both\"></div>\n",
    "</div>\n",
    "\n",
    "\\begin{align}\n",
    "    \\text {Fig. 3.   Linear and nonlinear problems are represented here respectively.}\n",
    "\\end{align}\n",
    "<br>\n",
    "On the other hand, kNN classifier is a nonlinear classifier. It does not draw a decision boundary to separate classes like in the linear case. Since it is looking the nearest neighbors it does not matter if one class surrounds other class(as shown in figure 3).\n",
    "\n",
    "\n",
    "<h2>Bias-Variance Tradeoff</h2>\n",
    "\n",
    "Bias is calculated like this:\n",
    "\\begin{align}\n",
    "\\sum_{d \\epsilon D} (P(c|d) - \\Gamma_D(d))^2\n",
    "\\end{align}\n",
    "\n",
    "where $\\Gamma_D(d)$ is prediction of the model. Bias would be large if the model makes too many mistakes. Generally, linear methods would have a high bias for nonlinear problems, because there is no perfect linear decision boundary for nonlinear problems. Bias is generally low for nonlinear problems, because it can predict accurately even though the problem is nonlinear.\n",
    "\n",
    "Variance is calculated like this:\n",
    "\n",
    "\\begin{align}\n",
    "\\sum_{d \\epsilon D} (\\Gamma_D(d) - E_D\\Gamma_D(d))^2\n",
    "\\end{align}\n",
    "\n",
    "where $E_D\\Gamma_D(d)$ is the expectation of whole training set. High variance means that the algorithm gives very different models for the different training samples of the same distribution. In this case, the variance is low for the linear classifiers, while it is high for nonlinear classifiers. Since different samples of the same distributions generally draw similar decision boundary, the variance would be low for linear classifiers. Since the decision from the nonlinear model can vary very quickly around the class boundaries, the variance is very high.\n",
    "\n",
    "Learning methods with very high variance are vulnerable for overfitting while high bias has low accuracy in some problems. Those tradeoffs should be considered while choosing a classification algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
