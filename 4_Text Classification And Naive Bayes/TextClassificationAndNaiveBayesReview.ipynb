{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification and Naive Bayes Review\n",
    "\n",
    "For the context of information retrieval, generally we retrieve the documents according to the terms in the query. However, there is a problem with this approach. Although a document is relevant to a query if it doesn't contain the terms in the query, we would not retrieve that document. \n",
    "\n",
    "To retrieve the documents, we can use the topic similarity between the documents and a query.  In order to do that, we classify the documents and rank the ones that have the most similar topic with the query. In text classification, the term class is used instead of the term topic.\n",
    "\n",
    "<h2>Text Classification</h2>\n",
    "\n",
    "Suppose that document in a Document space is $d$  $\\epsilon$ $\\mathbb{X}$ and a set of classes is $\\mathbb{C} = (c_1, c_2, ... c_j)$. Given the training set $\\mathbb{D}$ that is labeled as $\\langle d,c \\rangle$, where $\\langle d, c\\rangle$ $\\epsilon$ $\\mathbb{X}$ x $\\mathbb{C}$. \n",
    "\n",
    "For example:\n",
    "\n",
    "$\\hspace{3 cm}$ $\\langle d, c\\rangle = $  $\\langle$ Random number generators are very useful in developing Monte Carlo method simulations,  Computer Science $\\rangle$\n",
    "\n",
    "After running learning algorithm on training set $\\mathbb{D}$, we get a classification function or classification model $\\gamma$ that get input as a document and outputs its class:\n",
    "\n",
    "\\begin{align}\n",
    "\\gamma : \\mathbb{D} \\longrightarrow \\mathbb{C}\n",
    "\\end{align}\n",
    "\n",
    "This process is called supervised learning. Mathematical notation of this process described as $\\Gamma(\\mathbb{D}) = \\gamma$. \n",
    "\n",
    "As we previously explained, given a document, classification model returns a class. However, in reality, a document can have many classes, this type of problems called <i>any-of</i> problem. In our case, model returns one class, that's why this problem is called <i>one-of</i> problem.\n",
    "\n",
    "In text classification, the aim is to get high accuracy on the test data. Getting high accuracy on training set does not guarantee that the classifier is going to give good results for new data. The assumption in machine learning task is that the test data and the training data are samples from the same distribution.\n",
    "\n",
    "<h2>Multinomial Naive Bayes</h2>\n",
    "\n",
    "Probability of document d being in class c $P(c|d)$ is calculated like:\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\\begin{align}\n",
    "P(c|d) \\propto P(C) \\sum_{1 \\leq k \\leq n_d} P(t_{k}|c)\n",
    "\\end{align}\n",
    "<br>\n",
    "\n",
    "where $P(t_{k}|c)$ is the probability of term k being in class c, so the number of the term k occurring in class c divided by the number of documents class c can give us $P(t_{k}|c)$. P(c) is the probability of the class, it is the number of documents in class c over the number of all documents. If the number of terms is not enough to decide what class the document is(tie if same score), P(c) can help us to break the tie. That way we favor on a class that has more training examples. If we calculate $P(c|d)$ for every c, we choose the class with the highest probability for the document. The complexity of the algorithm is $O(|\\mathbb{C}| \\text{ } |V|)$, since we iterate through all the terms in the vocabulary for all classes c.\n",
    "\n",
    "<h2>Bernoulli Naive Bayes</h2>\n",
    "\n",
    "The second type of classifier is Bernoulli NB classifier. In this classifier we use binary independence model, which means we ignore the number of terms in the documents, we only need to know if the term occurred or not. This feature of Bernoulli NB classifier gives wrong results for long documents like books. Even though we have changed the algorithm, time complexity of NB classifier has not changed. \n",
    "\n",
    "One of the challenges in classification task is that in reality classes are not static which means the vocabulary of a class changes over time. We call this <i>concept drift</i>. Since the whole vocabulary will not change overnight, but it would gradually change in time, we know NB will not give the wrong results when concept drift occurs. Since NB classification algorithm is very fast and has high accuracy on large data, it is often picked first for the classification tasks.\n",
    "\n",
    "<h3>Feature Selection</h3>\n",
    "\n",
    "To get higher accuracy on NB classifier, we should eliminate unnecessary words from the corpus. This process is called feature selection. Since the vocabulary size is decreased, the running time of the algorithm would decrease. In order to do feature selection, we can use many algorithms such as mutual information, $\\chi^2$ and frequency-based feature selection. Mutual information tries to find how much information gain is provided by the term, if the inforation gain is . $\\chi^2$ method is generally used in statistics, it looks how much deviation from the expected number of terms. The last one is frequency-based feature selection, this one selects frequent words across the classes, if the word is common for all classes, it would not contain any information for a class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
