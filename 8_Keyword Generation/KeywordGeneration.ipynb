{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Keyword Generation for Search Engine Advertising Review\n",
    "\n",
    "In the previous <a href=\"https://github.com/alwaysprep/Independent-Study/blob/master/7_Predicting%20Clicks/Predicting%20Clicks.ipynb\">review</a>(predicting clicks), we discuss the benefit of predicting CTR from search engine standpoint. In this paper, we are going to discuss it from the advertiser point of view.\n",
    "\n",
    "Search engine advertising has become valuable for the advertsers as the internet has become an integral part of our daily lives. Since search engines become more efficient in targetted advertisement, and it has lowered the cost of the ads, the pay-per-click model has become the first choice amongst the advertisers. In the pay-per-click model, advertisers bid on the keyword phrases they want. The amount of bid affects where the ad is goint to be displayed. So the more you bid, the more probable your ad is ranked first place. Also, the inherent feature of bidding model increases the competition for explicit keywords. For example first position for \"cheap iphone\" might cost a few dollars, however, the term \"inexpenisive apple phone\" would cost a few pennies for the first position. Even though explicit keywords results in more CTR than nonobvious keywords, if the nonobvious keywords are chosen smartly, the total number of clicks would be similar to the ads with explicit keywords with much less cost. \n",
    "\n",
    "In this paper, authors introduce a new way to find nonobvious but synonymous keywords by using graph based techique called TermsNet.\n",
    "\n",
    "## Related Works\n",
    "\n",
    "In order to be on the first page of search engine results, websites use search engine optimization(SEO) techniques. One of the technique is to put relevant keywords to their meta-tags. To do that, a bot(meta-tag spider) queries to search engines and get meta-tags from the higher rank websites. \n",
    "\n",
    "Proximity based tools is another method to find relevant words to the keywords. It is using search engines and gets the words occur near to the keywords frequently. The drawback of this approach is that results are mostly nonrelevant words.\n",
    "\n",
    "To find relevant words search engines uses query logs analysis. The assumption here is that the most frequent queries that contain exact terms are probably good a candidate for high CTR ads. Even though it finds popular queries that result in high CTR, it fails to generate non-obvious keywords. There is one similar approach to this which is analyzing advertisers search log. This method also suffers the same problems with the previous one.\n",
    "\n",
    "\n",
    "## TermsNet\n",
    "\n",
    "When we think about similar words, the first thing comes to mind is using similarity metrics, like cosine similarity. However, terms are not suitible for cosine similarity. Authors use a different approach than similarity metrics. The primary assumption of their approach is that a term is as related to another term depending on how many times they ocur in the same documents. What this means is that, if term X is frequently appearing in documents containing term Y, we can say that term Y is relevant to term X. For example, if the term \"phone\" is generally appearing the documents contain the term \"Android\",  we can say that the term \"Android\" is relevant to the term \"phone.\" However, we can not say the other way around. Even though the term \"phone\" somewhat suggests the term \"Android\", it is not as relevant as the previous case.\n",
    "\n",
    "\n",
    "<img src=\"TermsNet.png\" width=\"40%\">\n",
    "\n",
    "\\begin{align}\n",
    "    \\text {Fig. 1. Directed graph of TermsNet.}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "By using this information we can create a directed graph as shown in Fig. 1. Nodes would represent the terms, edges would represent the relevance and edge weights would represent how strogly the term is suggesting the other term. If an edge goes to term Y from term X, it means that term X suggests term Y. One problem with that is some words occur most of the documents like \"stopwords\" and most of the terms suggest those words. That's why authors use idf to solve this problem. Simply the edge weights are divided by the logarithm of outdegre of the node.\n",
    "\n",
    "## Discussion\n",
    "\n",
    "For training set, they used eight thousand search terms from three different categoris such as travel, car rentals, and mortgage. Authors compare TermsNet with the tools we mentioned in related work. Comparizon parameters were relevance and nonobviousness. To compare the relevance of words they rely on human effort. To compare nonobviousness, they compared the stems of input keywords and output keywords by using a stemming algorithm. \n",
    "\n",
    "In my opinion trying to find nonobvious but relevant words for search engine advertisement is a very creative idea. However the tools that are compared with TermsNet are not built for this purpose and comparing it with them is a bit unfair. Also, experiments do not prove that those nonobvious keywords would be cheaper and have good CTR.\n",
    "\n",
    "The algorithm they use has very simplistic model for edge weights. I think, if they engineer their weights not just with the frequency parameter but with the distance parameter(how many words between terms), they will get more out of their model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
