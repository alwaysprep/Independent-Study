{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Language Models For Information Retrieval Review</h1>\n",
    "<br>\n",
    "\n",
    "$\\hspace{1 cm}$ As a user, we are aligning our vocabulary to the documents we want to find. In other words, we are imagining the <b>hypothetical relevant document</b> that we are trying to find. Out of this hypothetical document, we are generating the most likely query. Language models have the similar reasoning behind it: the document is relevant to the query if it is likely to create the query from its language model($M_d$). So instead of trying to find hte probability of relevancy by using probabilistic ranking principle(PRP), language models builds a probabilistic language model $M_d$ for every document $d$ and ranks documents by the likelihood of creating the query with its language model $P(q|M_d)$.\n",
    "<br>\n",
    "<br>\n",
    "<h2>Language Models</h2>\n",
    "\n",
    "$\\hspace{1 cm}$ The language model is trying to find how likely a specific string can occur for a given a \"language\". For example, the probability of this string (\"a quick brown dog\") occurring in English. If we know the documents' language models, we can find the probability of each document generating the query, and rank the probabilities. \n",
    "<br><br>\n",
    "<h4>Finite Automata in Language Models</h4></center>\n",
    "<br>\n",
    "If we have the language model we can get the probaility of generating each term from the language model. There is also the probability of ending the sentence.\n",
    "\n",
    "<br><br>\n",
    "\\begin{align}\n",
    "    P(q|M_d) = \\huge{[}\\normalsize\\prod_{q_t=1} P(t|M_d)  (1 - P(stop)) \\huge{]} \\normalsize P(stop)\n",
    "\\end{align}\n",
    "\n",
    "However since query length is constant for all documents we can eliminate stop probability. And equation becomes:\n",
    "\n",
    "\\begin{align}\n",
    "    \\normalsize\\prod_{q_t} P(t|M_d) \n",
    "\\end{align}\n",
    "<br><br>\n",
    "If you calculate the probability of generating the query for each document, you can rank the probabilities to get most relevant documents.\n",
    "\n",
    "<br><br>\n",
    "<h4>Types of Language Models</h4>\n",
    "<br><br>\n",
    "\n",
    "The most simple form of a language model is the unigram model. It is simply the product of all probability values for each term. This model assumes that terms are independent of each other. \n",
    "<br>\n",
    "\n",
    "\\begin{align}\n",
    "\\normalsize P_{uni}(t_1 t_2 t_3 t_4) = P(t_1) P(t_2) P(t_3) P(t_4)\n",
    "\\end{align}\n",
    "<br>\n",
    "There are some complex models for languages model one of them is:\n",
    "<br><br>\n",
    "\\begin{align}\n",
    "\\normalsize P_{bi}(t_1 t_2 t_3 t_4) = P(t_1) P(t_2|t_1) P(t_3|t_2) P(t_4|t_3)\n",
    "\\end{align}\n",
    "<br><br>\n",
    "Since IR does not dependent on the alignment of sentences, unigram language models generally works best for document ranking. Also, each model created for only one document there would not enough bigram data and probabilities converge to 0 very fast.\n",
    "<br><br>\n",
    "<h4>Multinomial Distribution of Language Models</h4>\n",
    "<br><br>\n",
    "\n",
    "Even though unigram model assumes that the ordering of words is not dependent of each other, this model gives the probability of specific ordering of words. \n",
    "\n",
    "<br><br>\n",
    "\\begin{align}\n",
    "\\normalsize P_{d} = \\frac {L_d!}{tf_{t_1}!tf_{t_1}!...tf_{t_M}!}  P({t_1})^{{t_1}} P({t_2})^{tf_{t_2}} ... P({t_M})^{tf_{t_M}}\n",
    "\\end{align}\n",
    "\n",
    "Since the left term is going to be same for every sentence the language model can generate, we can eliminate it.\n",
    "<br><br>\n",
    "<br>\n",
    "\n",
    "<h2>Query Likelihood Language Models</h2>\n",
    "\n",
    "The goal in query likelihood language model is to rank documents by $P(d|q)$. \n",
    "\n",
    "\\begin{align}\n",
    "P(d | q) = \\frac {P(q|d)P(d)} {P(q)}\n",
    "\\end{align}\n",
    "\n",
    "Since P(q) is same for every document, we can ignore it. Also, P(d) is uniformly distributed across documents it can be ignored too. So our equation becomes:\n",
    "\n",
    "\\begin{align}\n",
    "  = P(q|d) \\hspace{12 mm}\\\\\n",
    "\\end{align}\n",
    "\n",
    "From previous equation:\n",
    "\n",
    "\\begin{align}\n",
    "= \\normalsize\\prod_{t \\epsilon V} P(t|M_d)^{tf_{t,d}}\n",
    "\\end{align}\n",
    "<br><br>\n",
    "So we need to estimate query generation probability of a document $P(q|M_d)$. We can use maximum likelihood estimation for this: \n",
    "<br><br>\n",
    "\n",
    "\\begin{align}\n",
    " \\normalsize \\hat{P}(q|M_d) = \\prod_{t \\epsilon q} \\hat{P}_{mle}(t|M_d)= \\prod_{t \\epsilon q} \\frac{tf_{t,d}}{L_d}\n",
    "\\end{align}\n",
    "\n",
    "Although MLE is a good model for estimating the probability of query generation, we can have some words occur in the query but do not occur in the document which makes $\\hat{P}(q|M_d)=0$. So we need to smooth the probability values before getting the product. \n",
    "\n",
    "If t does not occur the in document but occurs in the query, We can say that $\\hat{P}(t|M_d) \\leq \\frac {cf_t}{T}$ where $cf_t$ is the term frequency over the corpus and T is the total number of words in the corpus. We can also weight ($ 1 - \\lambda$) which will determine, how much this corpus smoothing would effect our prior.\n",
    "<br><br>\n",
    "\\begin{align}\n",
    "\\hat{P}(t|d) = \\lambda \\hat{P}(q|M_d) + (1- \\lambda) \\hat{P}(t|M_c)\n",
    "\\end{align}\n",
    "<br><br>\n",
    "Where $ 0 \\lt \\lambda \\lt 1 $ and $M_c$ is a language model of the corpus. This method is called <i>linear interpolation</i> language model or Jelinek-Mercer smoothing.\n",
    "<br><br>\n",
    "\n",
    "So:\n",
    "\n",
    "\\begin{align}\n",
    "P(d|q) \\propto P(d) \\prod_{t \\epsilon q} \\lambda \\hat{P}(t|M_d) + (1- \\lambda) \\hat{P}(t|M_c)\n",
    "\\end{align}\n",
    "\n",
    "We assume that the P(d) is uniform throughout the corpus, so we can eliminate it. Now, we can calculate the probability that the document creating a query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Since testing the relevance rank and finding suitable datasets are very hard, I am not able to test Language Models in this notebook.\n",
    "<br>\n",
    "\n",
    "\n",
    "Unlike Probabilistic models, Language models do not need to know relevance information of documents beforehand. Although language models have promising results over Probabilistic models, it has still the same problem. The whole model should change for every query. That's why it is very hard to implement on large scale applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
